# Tests $LS$ and $LS^*$

<!-- Heading changed to Level one for the supplement -->

Test $LS$ was suggested by @Simonsohn:etal:2014 as a means of testing for "left skew," a "lack of evidential value," and "intense $p$ hacking" (p. 539-540). From the previous exposition, it should be obvious that the test is not a test of skew. It also cannot be a test of "evidential value" --- at least, not in the same sense as $EV$ --- because like $LEV$, test $LS$ will have overlapping rejection regions, and hence the same set can both have evidential value and lack evidential value under their interpretation of evidential value. 

Test $LS$ is better understood as a test of model fit. Under the assumptions of @Simonsohn:etal:2014 there is a family of possible distributions, all of which produce $p$ value distributions that are either uniform or are stochastically dominated by a uniform. Test $LS$ is statistically significant when $-2\sum_i^k\log(1-p_i)$ is much larger than expected under any possible combination of effect sizes.

<!--- Email from Simonsohn July 8, 2024


The reasoning [for eliminating test LS*] was that testing 'whether' there is selective reporting seemed silly. 

First, of course there is selective reporting.

Second, if there is, it does not matter much, what matters is whether the results are informative accounting/controlling for selective reporting. 

Third, some forms of selective reporting cause left-skew, others just uniform (https://datacolada.org/91). So it is arbitrary at least partially to deem somethingn significantly p-hacked if it has left-skew, for it picks up only slow p-hacking. It's really just a test of slow p-hacking.
--->
Interestingly, test $LS^*$ does not appear in @Simonsohn:etal:2015, where tests $EV^*$ and $LEV^*$ were introduced. Simonsohn et al have also removed $LS/LS^*$ from their online app without stating why; when asked, Simonsohn stated via email that "[t]he reasoning [for eliminating test LS*] was that testing 'whether' there is selective reporting seemed silly," because "of course there is selective reporting," and that it does not detect widespread forms of "p-hacking" [@simonsohnPersonalCorrespondenceEmail2024].

Despite the fact that test $LS^*$ was not described by Simonsohn et al (2014), it is easy to compute. Due to the symmetry of the normal, $p^*_{\ell,\cdot}=1-p^*_{e,\cdot}$, so the $p$ value may be inferred from their app results. In none of the `r sum(pcurve_use_summary$n)` $P$-curve analyses we harvested from the literature was $p^*_{e,\cdot}$ large enough to yield a significant test $LS^*$ (that is, $p^*_{e,\cdot}>.95$). If $LS^*$ is, in fact, a test of selective reporting and selective reporting is everywhere, $LS^*$ test does not appear to be very sensitive to whatever levels of selective reporting exist.

Because test $LS$ and $LS^*$ can be thought of as tests $LEV$ and $LEV^*$ but calibrated to $\lambda=0$ instead of $\lambda=2.34$, they will share many of the same properties. We therefore omit lengthy discussion of the specific proprties of these two tests. However, it is worth reiterating that these tests remain extremely sensitive to values near the significance criterion. If we interpret test $LS$ and $LS^*$ as "pure" tests of model fit, then this sensitivity should lead us to search for alternatives that are not affected by sensitivity. In fact, there is an obvious one, and we can use test $EV$'s $p$ value to construct it: $1-p_{e,\cdot}$ (i.e., reject when $p_{e,\cdot}$ is large, not small; we can call this test $EV^-$). This test will reject only when the $p$ values in the set are *consistently* higher than expected, because $p$ values near the boundary are mapped to $-2\log(1)=0$ by the transformation.

If tests $LS$, $LS^*$, and $EV^-$ are tests of model misfit, what might be the cause of such misfit? "Intense p-hacking" is Simonsohn et al's explanation, but other explanations are possible. Others include conservative tests (which generate larger $p$ values, on average, than a uniform when effect sizes are small); conservative behaviour [@erdfelderDetectingEvidentialValue2019]; and selection effects in gathering the studies (e.g., including a large $p$ value that made one initially suspicious in the $P$-curve analysis). 

The $p$ values gathered in a meta-analysis are never generated from a stochastic process; they were generated by a complex and convoluted human process that meanders through write-up, publication, and a search then selection by meta-analysts. If the statistical model underlying the $P$-curve were to misfit, any one of these might be the cause. We should be careful not to let our limited imagination of *possible* explanations keep us from being conservative about interpreting misfits.

Of all tests, we regard $EV^-$ as being potentially the most interesting, at least if it is carefully interpreted. As we have noted, the hypotheses underlying $EV/LEV$ and $EV^*/LEV^*$ are not terribly interesting; on the other hand, the particular kinds of misfit identified by tests $EV^-$ may be.
