
```{r hypos, eval=FALSE}
data.frame(
  tests = c('$EV, EV^*$','$LEV, LEV^*$','$LS, LS^*$'),
  nulls = c('$\\lambda=0$',"$\\lambda\\geq\\lambda'$","$\\lambda\\geq0$"),
  alts = c('$\\lambda>0$', '$\\lambda<\\lambda\'$', '$\\lambda \\in \\{\\}$'),
  interpretation = c('"Selective reporting ruled out as sole explanation for findings/evidential value" or "right skew"',
                     '"Effect is not larger than negligible/lack of evidential value" or "flatter than one would expect [if $\\lambda\\geq\\lambda\'$]"',
                     '"Intense $p$-hacking" or "left skew"')
) |>
  gt(caption = 'Tests and hypotheses used in $P$-curve analysis, where $\\lambda$ is a noncentrality parameter.') |>
  cols_label(
    tests = 'Test',
    nulls = md('$H_0$'),
    alts = md('$H_A$'),
    interpretation = "Simonsohn et al's (2014) interpretation"
  ) |>
  fmt_markdown() |>
  cols_width(
    interpretation ~ px(100)
  ) #|>
  #as_latex_with_caption('hypos', afterpage=TRUE)
```

There are two sets of methods Simonsohn et al. have called "$P$-curve": one set comprises meta-analytic significance tests, which we discuss in this manuscript.  @Simonsohn:etal:2014b also developed methods for estimating effect sizes which we will only mention in passing (though they are related to their method for estimating "average power", which we discuss later). For a critique of the second set of $P$-curve methods, see @McShane:etal:2016. Counterintuitively, in the context of the meta-analytic tests "$P$-curve" does not refer to the distribution of $p$ values [c.f. @ulrichPropertiesPcurvesApplication2018]; "$P$-curve" refers to tests based on *sums* of transformed $p$ values.

The basic model underlying the $P$-curve procedure is motivated by an assumption that selective reporting---in particular, the difficulty of publishing statistically nonsignificant findings---distorts the record of nonsignificant results in an unknown way, but that significant results provide for "unbiased inferences" [@Simonsohn:etal:2014, p. 535]. Thus, the first step in $P$-curve analysis is to collect test statistics for meta-analysis and then discard all test statistics that yield $p>\alpha_{pc}$, where the publication criterion (pc) is $\alpha_{pc}=.05$ by default. The statistically significant test statistics are treated as coming from a truncated distribution, where the truncation point is determined by the publication criterion.

@Simonsohn:etal:2014 and @Simonsohn:etal:2015 refer to three tests: a "test of evidential value/right skew^[When Simonsohn et al refer to \`\`right skew", they mean the skew of the \`\`true" distribution of $p$ values implied by underlying distributions' noncentrality parameters. This will be uniform when the null hypothesis is true and right-skewed when the null hypothesis is false under many, but not all, common tests. See @gelmanDiscussionDifficultiesMaking2014a for further critical discussion.]", a "test of lack of evidential value", and a "test of left skew". We refer to the 2014 version of these tests as $EV$, $LEV$, and $LS$; the 2015 versions, we refer to as $EV^*$, $LEV^*$, and $LS^*$. The distinction between them is meta-analytic, which we describe shortly.

The $P$-curve method is defined for a variety of common test statistics (e.g., $z$, $\chi^2$, $t$, $F$, $r$). Two-tailed $z$ or $t$ statistics are squared to yield $\chi^2$ or $F$ statistics, respectively. Two-tailed Pearson correlations are also converted to $F$ statistics. To demonstrate the method, we could choose either $\chi^2$ or $F$ statistics. We use $\chi^2_1$ test statistics for simplicity and because they will be familiar due to the link with the standard Normal. $F$ statistics have similar issues to the ones we describe (Section \ref{extension-to-f-statistics}).

Consider a random variable $X$ with a truncated noncentral $\chi^2_1$ distribution:
\[
X \sim \text{noncentral}\,\chi^2_1(\lambda,t_\alpha),
\]

where $\lambda$ is the noncentrality parameter and $t_\alpha$ is a left truncation value (e.g., the critical value for the test of $\lambda=0$, $F^{-1}_{\chi^2_1}(1-\alpha_{pc})\approx 3.84$ for $\alpha_{pc}=.05$ where $F$ is a cumulative distribution function). $EV$ and $EV^*$ test the null hypothesis that $\lambda=0$ against the alternative that $\lambda>0$. Simonsohn et al. (2014a) interpret a rejection as indicating that "selective reporting [is] ruled out as sole explanation for findings"/"evidential value" (p. 535).

Tests $LEV$ and $LEV^*$ test the null hypothesis that $\lambda\geq\lambda'$, where $\lambda'$ is chosen as the noncentrality parameter that would lead to a $1/3$ probability of statistical significance. For $\chi^2_1$ test statistics, $\lambda'=2.34$. The $1/3$ probability of significance is an arbitrary value selected by Simonsohn et al. (2014a), described as "merely a suggestion" (p. 538). The value of $1/3$ is hard-coded in their online app and cannot be changed. In reviewing papers that cited Simonsohn et al. (2015), we did not encounter any authors that opted for a different value. 

Test $LS$ is identical to test $LEV$ except that it tests the less arbitrary null hypothesis that $\lambda\geq0$; that is, it will reject when the observed test statistic is small under every possible noncentrality parameter. It is thus a test of the fit of the whole model. The analogous test $LS^*$ was never developed by the authors and test $LS$ was dropped without explanation from their online app. The properties of the tests will be similar to $LEV/LEV^*$, so we will not focus on these tests directly (but see the Supplement for additional commentary).

Let $p_e$ and $p_n$ refer to the $p$ values for tests $EV$ and $LEV$ respectively ("$e$" for "evidential value", "$n$" for "no [lack of] evidential value") for a single study. Here $p_e=p/\alpha_{pc}$ is simply the $p$ value from the original test renormalized by $\alpha_{pc}$ to account for the truncation, and is small when $X$ is large. $p_n$ is the probability of obtaining a larger $p$ value than the one observed, but still less than $\alpha_{pc}$, given noncentrality parameter $\lambda'$: $p_{n}=(F_{\chi^{2}_{1}(\lambda')}(X)-F_{\chi^{2}_{1}(\lambda')}(t_\alpha))/(1-F_{\chi^{2}_{1}(\lambda')}(t_\alpha))$. $p_n$ is small when $X$ is near the significance criterion, approaching 0 at $\alpha_{pc}$.

Tests $EV$ and $EV^*$ dramatically increase the evidence required to argue for an effect: a single study's results must be surprising even among significant studies ($p<\alpha_{pc}^2$) in order to be said to have "evidential value". This is the natural effect of taking statistical significance for granted due an extreme file-drawer effect. On the other hand, $p$ values near the significance criterion are potentially flagged as problematic by the remaining tests.

Consider a sequence of statistically significant study results $X_i$, $i=1,\ldots,K_\alpha$ modeled as independent truncated $\chi^2_1$ variates with noncentrality parameters $\lambda_i$, and let $K_\alpha$ be the number of results that would be significant at level $\alpha$. We drop $\alpha$ by default to mean all results significant at the conventional $\alpha=0.05$ level. Define $p$ values $p_{e,i}$, $p_{n,i}$ analogously as $X_i$. 

Simonsohn et al. use two methods to combine the $p$ values from the individual tests into a single test statistic. In 2014 they use Fisher's method of summing the logarithms of $p$ values (as do @vanAssen:etal:2015, who suggest test $EV$ at around the same time); in 2015 they use Stouffer's method of summing probit-transformed $p$ values (see @mardenCombiningIndependentOneSided1985 p. 1536 for a description of various meta-analytic methods for combining $p$ values). Under the null hypothesis $\lambda_i=0, \forall i$ the resulting sums have central $\chi^2_{2K}$ and Normal distributions, respectively; $p$ values are computed relative to these null distributions:

\begin{align*}
p_{e,\cdot} &= 1-F_{\chi^2_{2K_\alpha}}\left(-2\displaystyle\sum_{i=1}^{K_\alpha} \log(p_{e,i})\right)  &\text{(Simonsohn et al., 2014)} \\
p^*_{e,\cdot} &= \Phi\left(\frac{1}{\sqrt{K_\alpha}}\displaystyle\sum_{i=1}^{K_\alpha} \Phi^{-1}(p_{e,i})\right) & \text{(Simonsohn et al., 2015)} 
\end{align*}
where $\Phi$ and $\Phi^{-1}$ denote, respectively, the cumulative distribution function of the univariate normal distribution and its inverse. We can define $p_{n,\cdot}$ and $p^*_{n,\cdot}$ analogously. For demonstration purposes, see our interactive app that performs the $P$-curve tests at https://richarddmorey.github.io/pcurveAppTest/.

For the meta-analytic tests $EV$ and $EV^*$ the null hypothesis tested is that $\lambda_i=0$ in all studies; hence, the alternative is that $\lambda_i>0$ for at least one $i$. For $LEV$ and $LEV^*$, the null hypothesis is that all $\lambda_i\geq\lambda'$; hence, a rejection would typically be interpreted as favoring the alternative $\lambda_i<\lambda'$ for at least one $i$.

Simonsohn et al's (2015) change from summing logarithms (2014) to summing probits (2015) was accompanied by a change in their online app [@simonsohnPcurveApp062017], which no longer offers the 2014 tests. Citing @Abelson:1995, their justification was that the probit combination method is "less sensitive to a few extreme results" (p. 1149), a property they preferred. This choice has dramatic implications for the statistical properties of the method, to which we turn later.

A second change that @Simonsohn:etal:2015 made was to define a procedure including the "full" $P$-curve, as described above *and* a "half" $P$-curve, which is defined analogously but the truncation point is $p<\alpha_{pc}/2$ instead of $\alpha_{pc}$. They argue this accounts for "aggressive $p$ hacking": that is, that some people's target significance criterion when engaging in bad behaviour may be lower than 0.05. They (arbitrarily) choose half the usual criterion as a secondary criterion and build a new test procedure: "We introduce the following novel test of evidential value: *A set of studies is said to contain evidential value if either* the half $P$-curve has a p < .05 right-skew test, *or* both the full and half $P$-curves have p < .1 right-skew tests." [@Simonsohn:etal:2015, p1151, emphasis in original] For most of this manuscript we do not use this compound test procedure, instead concentrating on each test on its own. We return to the compound criterion later to show that it leads to nonmonotonicity in the evidence.

### Use of the methods in the literature

```{r}

# total papers in citation list, as of July 2024
npapers_cite = 186

here::here('data/pcurve_review_with_meta.csv') |>
  read.csv() |>
  mutate(
   # across(matches('.p$'), ~stringr::str_remove_all(.,'[^0-9.]{1,}') |> as.numeric()),
  ) -> pcurve_use_df0

pcurve_use_df0 |>
  group_by(has_results=!is.na(EV.p) | !is.na(LEV.p), k_known=!is.na(k)) |>
  summarise(n = n()) -> n_valid

pcurve_use_df0 |>
  mutate(both_missing = is.na(EV.p) & is.na(LEV.p)) |>
  filter(!both_missing  & !is.na(k) ) -> pcurve_use_df


pcurve_use_df |>
  group_by(Paper) |>
  summarise(n = n()) |>
  nrow() -> npapers1

pcurve_use_df |>
  group_by(EVsig=EV.p<=.05,LEVsig=LEV.p<=.05) |>
  summarise(
    n=n(),
    k_med = median(k),
    k_range = glue::glue("({min(k)}-{max(k)})")
    ) |>
  mutate(p_giv_EV = n / sum(n)) |>
  ungroup() |>
  mutate(
    p_marg = n / sum(n),
    across(matches('sig'), ~ case_when(
      is.na(.) ~ 'Did not report',
      . ~ '$p<0.05$',
      TRUE ~ '$p\\geq0.05$')
    )
    ) -> pcurve_use_summary

qk = summary(pcurve_use_df$k)[-4]
names(qk) = c("Min.","Q1","Median","Q3","Max.")

k_by_sig = tapply(pcurve_use_df$k,pcurve_use_df$EV.p<.05,median)

```

```{r revresults}

pcurve_use_summary |>
  mutate(EVsig = case_when(
    row_number()%%3 == 2 ~ EVsig,
    TRUE ~ ''
  )) |>
  select(-p_giv_EV) |>
  relocate(matches('k'), .after='p_marg') |>
  gt(caption = 'Cross tabulation of the results of P-curve analyses in papers that cite Simonsohn et al. (2015).') |>
  fmt_percent(matches("^p"),decimals = 1) |>
  fmt_passthrough(matches('sig'),escape = FALSE) |>
  cols_label(
    EVsig = 'EV*',
    LEVsig = 'LEV*',
    n = 'N',
    #p_giv_EV='given EV* result',
    #p_marg='of total',
    p_marg = '%',
    k_med = 'Med.',
    k_range = 'Range'
  ) |>
  tab_spanner("Test",1:2) |>
  #tab_spanner("%",6:7) |>
  tab_spanner(label="K",columns = 5:6) |>
  cols_align(align = 'right',3:6) #|>
#  tab_style(
#    locations = cells_body(),
#    style = cell_borders(sides='all',color='#ffffff',weight = px(0))
#  ) |>
#  tab_style(
#    locations = cells_body(columns=2:6,1:5),
#    style = cell_borders(sides='bottom',color='#dddddd',weight = px(1))
#  ) |>
#  tab_style(
#    locations = cells_body(columns=1:6,3),
#    style = cell_borders(sides='bottom',color='#dddddd',weight = px(2.5))
#  ) |>
#  as_latex_with_caption('revresults', afterpage=TRUE,spacing=1)
```

As of July 2024, Simonsohn et al. (2014; log method) and Simonsohn et al (2015; probit method) collectively have about 1200 citations, most of which are applications of the method. We reviewed all $P$-curve tests in the `r npapers_cite` papers that cited Simonsohn et al. (2015); see Table \ref{tab:revresults} for a summary. More details can be found in the Supplement. The vast majority of applications of $EV^*$ were significant (`r round(sum(pcurve_use_summary$p_marg[4:6])*100,1)`% of $P$-curves), and the modal conclusion from applying $P$-curve methodology was "evidential value" for whatever set was reported. Only rarely (`r round(sum(pcurve_use_summary$p_marg[2])*100,1)`%) did a $P$-curve analysis yield a statistically nonsignificant $EV^*$ test and a significant $LEV^*$ test (supposedly indicating an "lack of evidential value").

