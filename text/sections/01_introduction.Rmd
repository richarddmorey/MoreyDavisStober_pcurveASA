
Forensic meta-analysis methods are designed to assess the quality of the evidence in sets of results. Unlike typical meta-analysis, the aim is not substantive; rather, forensic meta-analysis questions the evidence itself. @Simonsohn:etal:2014 and @Simonsohn:etal:2015 describe *$P$-curve* analysis, a suite of statistical tests to evaluate the "evidential value," or lack thereof, of collections of studies, potentially in the presence of poor researcher behaviors. These poor behaviors---collectively referred to as $p$-hacking [@Simmons:etal:2011]---include selective reporting, *post hoc* data exclusion, and related data manipulation carried out with an aim of producing statistical significance ($p<.05$) that otherwise would not have been found.

The $P$-curve method has been widely applied to assess the evidence for individual phenomena [e.g., @Simmons:Simonsohn:2017], collections of related empirical phenomena [e.g., @hosseini2021differential;@cadario2020healthy], and even whole sub-fields, such as experimental philosophy [@stuart2019p]. Breaking out of the academic arena, the $P$-curve played a major role in a New York Times Magazine story: @dominusWhenRevolutionCame2017 outlines how @Simmons:Simonsohn:2017 claim the evidence for a "power posing" effect---whereby adopting an expansive physical posture increases risk taking and testosterone levels, and decreases cortisol levels [@carney2010power]---is lacking, based on a $P$-curve analysis of 34 studies. In response, @cuddy2018p carried out another $P$-curve analysis on a larger set of 55 studies including the original 34 studies and found "clear" evidential value. The $P$-curve method is among the most popular of forensic statistical procedures. Given its widespread use, it is crucial that the $P$-curve procedure be sound.

$P$-curve analysis is motivated by the distribution of statistically significant $p$ values under three possible conditions:  First, under a null hypothesis where there is collectively no effect among the many $p$ values from studies being evaluated, the $p$ values will often be uniformly distributed on the $[0,.05]$ interval.^[It should be noted that the condition for a valid $p$ value is not uniformity, but that the $p$ values are uniform *or* stochastically dominate a uniform. The $P$-curve authors assume that tests are not conservative.]  Second, if the null hypothesis is false, $p$ values will tend to gather around 0 (what the authors dub "right-skew").  Finally, if the null hypothesis is true and the researchers who carried out the original tests engaged in $p$-hacking or similar behavior to lower the reported $p$ values across the threshold of significance (e.g. $\alpha=.05$) then the distribution of significant $p$ values may tend to gather just below the significance threshold ("left-skew").  The $P$-curve statistical methodology is carried out in an attempt to adjudicate between these three conditions.

Others have criticized the $P$-curve method of @Simonsohn:etal:2014 and @Simonsohn:etal:2015 from various angles. @Ulrich:Miller:2015 argue that the original $P$-curve tests [@Simonsohn:etal:2014] fail to detect when researchers selectively report the outcomes of multiple tests, i.e., a researcher running multiple tests but only reporting the result with the smallest $p$ value; [c.f. @Simonsohn:etal:2015]. @erdfelderDetectingEvidentialValue2019 present conditions under which right-skewed distributions of $p$ values are not diagnostic of evidential value, and conservative reporting practices under which left-skewed distributions of $p$ values are not diagnostic of $p$-hacking behavior; see also @bishop2016problems. Finally, @montoyaInconsistencyPcurveTesting2024 show that when there are multiple choices of which $p$ value from a study to enter into a $P$-curve analysis, this choice has an impact on the analysis, despite it often being arbitrary.

These critiques are important. Our approach in critiquing $P$-curve analysis, however, is different: we examine the soundness of the statistical procedure itself.  We follow two broad lines of argumentation. In Section \ref{conceptual-issues-with-the-p-curve}, we demonstrate that the statistical tests comprising $P$-curve analysis are logically disconnected from the arguments presented in \citet{Simonsohn:etal:2014} and \citet{Simonsohn:etal:2015}, which leads to poor test behavior and unsupported claims. In the second line of argumentation we show that *even if* the general interpretation of the tests were sensible, the tests have poor statistical properties. These poor properties include extreme sensitivity (Section \ref{undue-sensitivity-at-the-critical-boundary}), test inadmissibility (Section \ref{inadmissibility}), and, perhaps most disturbing, nonmonotonicity in the evidence (Section \ref{nonmonotonicity-in-the-evidence}). 
