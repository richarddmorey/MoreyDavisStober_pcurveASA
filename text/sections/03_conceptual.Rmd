
Before our formal statistical critiques of the $P$-curve, we offer a number of critiques centering on its conceptualization and use.

**Absurd repeatable process.** The repeatable process assumed by the $P$-curve authors (and their power analyses) uses a fixed number of significant results for all repetitions.  Given that $p$ values are random variables, it is quite strange to assume that a given number of draws from a distribution of $p$ values will yield \emph{exactly} the same number of $p$ values less than .05 (both $p$-hacked and non-$p$-hacked) each time.  This naturally leads to absurdities. If we take this assumption at face value, then researchers would also be $p$-hacking to non-significance (i.e., above .05) to maintain a fixed number of significant $p$ values across repetitions. The important critique here is that the $P$-curve authors do not consider plausible models of researcher behavior and hence ignore an important source of variability.

**Not tests of skew.** Simonsohn et al (2014a, 2015) describe their tests in terms of the skew of the observed $p$ values (i.e. "$P$-curves that are not right-skewed
suggest that the set of findings lacks evidential value," Simonsohn et al, 2014a, p. 535). However, the $P$-curve tests are all based on sums of transformed $p$ values. There are many sets consistent with a given sum: for instance, any sum of $K$ values is consistent with a set of $K$ identical values. This set has no skew. Manipulation of this set of identical values can yield sets of left or right skew with the same sum. The test is only responsive to the size of the sum, not the skew of the values within it. It is true that highly right-skewed samples of $p$ values often lead to rejections of \(EV^*\), but left-skewed samples can do the same. 

Given that the theoretical distribution of $p$ values when at least one $\lambda_i>0$ is right-skewed (assuming no poor researcher behavior), one might be tempted to consider a test of all $\lambda_i=0$ with the alternative at least one $\lambda_i>0$ a test of skew. In this case, there is nothing special about skew; we could just as easily regard the $P$-curve tests as tests of the expected $p$ value (because the expected value of the $p$ values decrease as some $\lambda_i$ increases), or the variance (which decreases as some $\lambda_i$ increases). 

Moreover, under the assumptions of Simonsohn et al (2014a, 2015), there are no effect sizes that would lead to *left* skew (i.e. a preponderance of $p$ values near the publication threshold). Thus tests of $\lambda$ cannot be tests of left skew, despite that they advertise a test of left skew ($LS$).

Taken together, the authors' use of the concept of skew is inconsistent with both sample skew and population skew. The $P$-curve tests are simple tests of noncentrality parameters; introducing the concept of skew obfuscates how the tests work.

**Noncentrality parameters, not effect sizes.** Tests of noncentrality parameters are not equivalent to tests of effect sizes. Given that the input to the $P$-curve procedures is a set of $p$ values, and one persistent critique of $p$ values is that they confound effect size and sample size [e.g. @Berkson:1938 onward], one might wonder how it is that the $P$-curve tests can be tests of effect size.  In truth, they are not. 

When testing the null hypothesis that the effect size is 0, we can elide the difference because when the noncentrality parameters are 0, so are the effect sizes. Tests $EV$ and $EV^*$ are tests of whether the average transformed $p$ value is what would be expected if all considered effect sizes are exactly zero, with an alternative that at least one noncentrality parameter $\lambda_i>0$. 

Tests $LEV$ and $LEV^*$, on the other hand, are tests of whether the average transformed $p$ value is what would be expected if the $\chi^2_1$ noncentrality parameters, from all considered studies, are all larger than 2.34 (*mutatis mutandis* for other test statistics). In a typical two-tailed $z$ test (performed as a $\chi^2$ test from which the $X_i$ values may derive) the noncentrality parameters are $\lambda_i = \delta_i^2N_i$, where $\delta_i$ and $N_i$ are standardized effect size and effective sample size, respectively. The hypothesis being tested by $LEV$ and $LEV^*$ is dependent on the design of the studies at hand: they are tests whose alternative hypothesis is that "for *at least one* study $i$, $|\delta_i|<\sqrt{2.34/N_i}$." This is in contrast to Simonsohn et al.'s (2014a) claim that $LEV$, for instance, is a test "not that the effect is zero, but that it is very small instead" (p. 537). Conflating effect size with noncentrality parameters obfuscates what the hypotheses actually are; moreover, there is not a single effect size as implied by "*the* effect size": each study will have its own noncentrality parameter.

**Not about the "power" in a set.** @Simonsohn:etal:2014 claim that tests $LEV$ and $LEV^*$ are "test[s] for power of 33%" (p. 537). In a companion paper, @Simonsohn:etal:2014b suggest that that the "$P$-curve can be used to estimate the average underlying statistical power of a set of studies" (p. 676; though it should be noted that the $P$-curve analysis in that paper is slightly different, though related, to the $P$-curve analysis we discuss here). There are two reasons why this is not true. The first is conceptual: their "power" is a function of the true effect size, which is at odds with the classical concept of power (a *function* of *counterfactual*, not true, effect sizes). The second is statistical: their estimate is inconsistent, even if taken at face value. We address the latter critique in Section \ref{powerEstimation}.

**Logical disconnect between statistical hypotheses and "evidential value".** It is common for authors to interpret the results of the $P$-curve procedures in terms of the *set* of studies, of a broader research area, or in terms of a theoretical construct (like a psychological "effect"). Researchers may even split a meta-analysis into subsets and use $P$-curve to look for where the evidential value lies [e.g. @stuart2019p]. 

When rejecting the null hypotheses of $EV$, $EV^*$ the natural conclusion is that *at least one noncentrality parameter is non-zero*.  Likewise, when rejecting the null hypotheses of $LEV$, $LEV^*$ the natural conclusion is that *at least one noncentrality parameter* is less than 2.34 (c.f. Section \ref{properties-of-tests-lev-and-lev}). The $P$-curve, however, is intended to draw conclusions regarding properties of *sets* of studies; these null and alternative hypotheses, which still involve individual studies, are clearly logically disconnected from this purpose.  For any of the six tests, the only proper conclusion to draw is that it is not the case that *all* the studies have the property in question. 

If a single study in a set of 50 truly had a non-zero noncentrality parameter, would it be valid to claim that, for example, the corresponding literature from which the set is drawn is high-quality? Clearly not; however, it is common for $P$-curve practitioners to reject $EV$ or $EV^*$ and come to a similar conclusion. For instance, @wildeNeonatalJaundiceAutism2022 points to a statistically significant $EV^*$ test and writes "We should think of this result as a quality control stamp supporting the validity of the widely-noted correlation [between jaundice and autism]" (p. 23). @ruszRewarddrivenDistractionMetaanalysis2020 claims that a significant $EV^*$ test means that "this set of studies contains evidential value for reward-driven distraction." (p. 886).

The problem is analogous for $LEV$ and $LEV^*$: should a single study with a small sample size and smaller-than-expected noncentrality parameter "contaminate" an otherwise robust set of studies? For all tests, the use of a sum in computing the meta-analytic test statistic ensures this contamination will happen, but the typical conclusions based on that sum are far removed from what is warranted.

It is also worth pointing out that "evidential value" is not a well-defined statistical or scientific concept and the $P$-curve tests can come to contradictory conclusions regarding it. It is possible for test $EV/EV^*$ to reject and for $LEV/LEV^*$ to reject at the same time (see @westPCurveAnalysisTaylor2021 for a real example in the literature). A set surely cannot both contain evidential value and lack it.



